{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Generative Adversarial Networks (GANs)\n",
    "======================\n",
    "\n",
    "Artigo original: [Goodfellow et al (2020)](https://dl.acm.org/doi/abs/10.1145/3422622)\n",
    "\n",
    "Duas redes generativas \"competem\" em um contexto determinado.\n",
    "\n",
    "A rede Generativa, por meio de um vetor D-dimensional $V_{noise}$ gera imagens \"falsas\", as quais são embaralhadas a imagens reais.\n",
    "A rede Discriminativa deve identificar as imagens reais e as imagens falsas."
   ],
   "id": "bd51f74d8b114435"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-25T12:11:48.134061Z",
     "start_time": "2024-08-25T12:11:48.129194Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Espaço latente\n",
    "===============\n",
    "\n",
    "Espaço latente é um espaço de dimensão reduzida onde os dados são representados de maneira compacta e significativa. Os vetores latentes são pontos neste espaço. Cada vetor latente pode ser mapeado para uma amostra no espaço original dos dados (por exemplo, uma imagem, texto ou som)."
   ],
   "id": "2912446a02e07e93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Vetor latente\n",
    "______________\n",
    "\n",
    "1. **Representação Compacta:** Um vetor latente é uma representação compacta e densa de uma amostra. Por exemplo, uma imagem de alta resolução pode ser representada por um vetor latente de dimensão muito menor do que o número total de pixels da imagem.\n",
    "\n",
    "2. **Dimensionalidade Reduzida:** O vetor latente possui uma dimensionalidade menor em comparação com os dados originais, o que ajuda a capturar os padrões mais importantes dos dados. Isso significa que o vetor latente retém as informações mais relevantes de maneira condensada.\n",
    "\n",
    "3. **Entrada para Modelos Geradores:** Em modelos como GANs, o vetor latente serve como entrada para o gerador, que transforma esse vetor em uma amostra do espaço original (como uma imagem, som ou texto). Diferentes vetores latentes podem gerar diferentes amostras, e pequenas mudanças no vetor latente podem resultar em variações na amostra gerada."
   ],
   "id": "4ca79a6112d2d095"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T12:10:13.559509Z",
     "start_time": "2024-08-25T12:10:13.556027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parâmetros\n",
    "latent_dim = 100"
   ],
   "id": "4b3b402647416b8b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "latent_dim = 100\n",
    "=================\n",
    "\n",
    "* **Dimensão do Espaço Latente:** O número 100 indica que o vetor latente tem 100 componentes ou dimensões. Isso significa que cada vetor latente `z` é um vetor de comprimento 100, onde cada componente é geralmente um número amostrado de uma distribuição normal padrão (distribuição Gaussiana com média 0 e desvio padrão 1).\n",
    "* **Complexidade e Variedade:** O valor de 100 foi escolhido para fornecer ao gerador uma quantidade suficiente de variabilidade e capacidade para gerar uma ampla gama de imagens diferentes. Um vetor latente de alta dimensionalidade pode capturar mais nuances e detalhes nas amostras geradas, o que é útil para produzir imagens de alta qualidade e diversidade.\n",
    "\n",
    "* **Equilíbrio:** 100 é um valor comumente usado em muitas aplicações práticas de GANs, porque ele fornece um bom equilíbrio entre a capacidade do gerador de aprender padrões complexos e a eficiência computacional. Vetores latentes muito pequenos (por exemplo, 10 ou 20 dimensões) podem limitar a expressividade do gerador, enquanto vetores muito grandes (por exemplo, 1000 ou mais dimensões) podem tornar o treinamento mais difícil e instável, além de exigir mais recursos computacionais."
   ],
   "id": "dfcceb721a557ab1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T12:10:13.683483Z",
     "start_time": "2024-08-25T12:10:13.679736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img_shape = (1, 28, 28)\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "learning_rate = 0.0002"
   ],
   "id": "cc9849426a0f925c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T12:10:15.782310Z",
     "start_time": "2024-08-25T12:10:13.728401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dispositivo (CPU ou GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "676fa9d4c0f37161",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T12:10:15.834246Z",
     "start_time": "2024-08-25T12:10:15.829979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pipeline para transformação de dados\n",
    "transform = transforms.Compose([\n",
    "    # Transformação para tensores de entrada. Cores RGB variam de [0, 255]. Transforma-se para variarem entre [0.0, 1.0]\n",
    "    transforms.ToTensor(),\n",
    "    # Normaliza-se os tensores, subtraindo de um mean = [0.5] e divide-se de um std = [0.5]. Os valores passam a varia entre [-1.0, 1.0]\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])"
   ],
   "id": "533be43b8f6361bd",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T12:10:15.963245Z",
     "start_time": "2024-08-25T12:10:15.878221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Carregar dataset MNIST\n",
    "dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ],
   "id": "a3686fbb58d76f91",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Gerador (Generator)\n",
    "====================\n",
    "\n",
    "* **Função:** Cria dados falsos que se pareçam com os dados reais. Ele recebe como entrada um vetor de ruído (também chamado de vetor latente), que é geralmente amostrado de uma distribuição simples, como uma distribuição normal. O gerador transforma esse vetor de ruído em uma amostra no espaço dos dados, como uma imagem, um texto, ou outro tipo de dado que se deseja gerar.\n",
    "\n",
    "* **Estrutura:** O gerador é uma rede neural que geralmente contém camadas como `Linear`, `ConvTranspose2d`, `ReLU`, `BatchNorm`, etc. Essas camadas têm como objetivo transformar o vetor latente em uma representação complexa que, eventualmente, se torna uma amostra de dados semelhante às amostras reais do conjunto de treinamento.\n",
    "\n",
    "* **Objetivo:** O objetivo do gerador é enganar o discriminador, fazendo com que ele classifique as amostras geradas como \"reais\". Durante o treinamento, os parâmetros do gerador são ajustados para maximizar a probabilidade de o discriminador classificar suas saídas como reais.\n",
    "\n",
    "* **Função de Perda:** A função de perda do gerador geralmente tenta maximizar o valor logarítmico da probabilidade do discriminador classificar as amostras geradas como reais, ou, alternativamente, minimizar o valor logarítmico da probabilidade de o discriminador classificá-las como falsas."
   ],
   "id": "9359f2418191013b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T12:10:15.978073Z",
     "start_time": "2024-08-25T12:10:15.972820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Gerador\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128 * 7 * 7),\n",
    "            nn.BatchNorm1d(128 * 7 * 7),\n",
    "            nn.ReLU(True),\n",
    "            nn.Unflatten(1, (128, 7, 7)),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)"
   ],
   "id": "cb698e2861b029f6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Discriminador (Discriminator)\n",
    "============================\n",
    "\n",
    "* **Função:** O discriminador é responsável por distinguir entre amostras reais (do conjunto de treinamento) e amostras falsas (geradas pelo gerador). Ele recebe uma amostra como entrada e retorna uma probabilidade de que a amostra seja real (ou seja, pertencente ao conjunto de dados real) ou falsa (ou seja, gerada pelo gerador).\n",
    "\n",
    "* **Estrutura:** O discriminador também é uma rede neural, mas geralmente é uma rede de classificação binária. Ele pode incluir camadas como `Conv2d`, `LeakyReLU`, `Flatten`, `Linear`, `Sigmoid`, etc. Essas camadas são projetadas para extrair características da entrada e decidir se a amostra é real ou falsa.\n",
    "\n",
    "* **Objetivo:** O objetivo do discriminador é identificar corretamente se uma amostra é real ou falsa. Durante o treinamento, os parâmetros do discriminador são ajustados para maximizar a precisão na classificação de amostras reais e falsas.\n",
    "\n",
    "* **Função de Perda:** A função de perda do discriminador é geralmente a soma das perdas de classificação das amostras reais e falsas. Ele tenta maximizar a probabilidade de classificar corretamente as amostras reais como reais e minimizar a probabilidade de classificar erroneamente as amostras falsas como reais."
   ],
   "id": "48eac67a072c7299"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T12:10:16.028547Z",
     "start_time": "2024-08-25T12:10:16.020223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Discriminador\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 7 * 7, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n"
   ],
   "id": "5847f2b035d007bc",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T12:10:16.250377Z",
     "start_time": "2024-08-25T12:10:16.074506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instanciar os modelos\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)"
   ],
   "id": "471e56ed15ef5193",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Momentos do ADAM\n",
    "================\n",
    "\n",
    "$$ Adam = AdaGRAD + RMSProp $$ \n",
    "\n",
    "AdaGRAD = lida bem com a espasidade dos dados;\n",
    "\n",
    "RMSProp = lida bem com gradientes explodindo ou desaparecendo.\n",
    "\n",
    "O Adam calcula momentos de 1ª e 2ª ordem dos gradientes para adaptar a taxa de aprendizado de cada parâmetro.\n",
    "\n",
    "1ª Ordem = médias móveis dos gradientes;\n",
    "\n",
    "2ª Ordem = quadrado dos gradientes.\n",
    "\n",
    "Parâmetros `beta` do Adam:\n",
    "\n",
    "`beta1 = 0.5`\n",
    "\n",
    "Controla a taxa de decaimento exponencial para o cálculo do momento de primeira ordem (a média dos gradientes).\n",
    "\n",
    "* O momento de primeira ordem é basicamente uma média móvel dos gradientes ao longo do tempo.\n",
    "* Um valor de beta1 mais próximo de 1 dá mais importância aos gradientes passados, acumulando uma média mais estável, mas que responde mais lentamente às mudanças nos gradientes.\n",
    "* Um valor de beta1 mais baixo (como 0.5) permite que o otimizador responda mais rapidamente às mudanças no gradiente, o que pode ser útil em situações onde o treinamento é instável, como no caso de GANs.\n",
    "* Por que 0.5?: Em GANs, é comum usar beta1 = 0.5 porque isso ajuda a estabilizar o treinamento. O treinamento de GANs pode ser instável e o ajuste desse valor para 0.5 ajuda a reduzir oscilações indesejadas no aprendizado.\n",
    "\n",
    "`beta2 = 0.999`\n",
    "\n",
    "Controla a taxa de decaimento exponencial para o cálculo do momento de segunda ordem (a variância dos gradientes).\n",
    "\n",
    "* O momento de segunda ordem é uma média móvel do quadrado dos gradientes.\n",
    "* Um valor de beta2 mais próximo de 1 significa que o otimizador considera a maioria das iterações anteriores para calcular a variância, o que suaviza as atualizações do modelo.\n",
    "* Um valor de beta2 mais baixo faria com que o otimizador respondesse mais rapidamente a mudanças na variância dos gradientes.\n",
    "* Por que 0.999?: Este é um valor padrão que funciona bem na maioria dos casos. Ele assegura que as estimativas de segunda ordem sejam estáveis e que o modelo não faça grandes mudanças com base em flutuações temporárias nos gradientes."
   ],
   "id": "a2d8131bc94ab5d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T12:10:16.262065Z",
     "start_time": "2024-08-25T12:10:16.258235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Função de perda e otimizadores\n",
    "criterion = nn.BCELoss().to(device)\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))"
   ],
   "id": "55ee2c3bbc4b0f1e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Interação entre Gerador e Discriminador - Adversarial learning\n",
    "=====================================\n",
    "\n",
    "1. **Treinamento Adversarial:** O treinamento de uma GAN envolve um jogo entre o gerador e o discriminador:\n",
    "\n",
    "* O gerador tenta melhorar suas amostras para enganar o discriminador.\n",
    "* O discriminador tenta melhorar sua capacidade de distinguir entre amostras reais e falsas.\n",
    "\n",
    "2. **Ciclo de Treinamento:**\n",
    "* Discriminador: É treinado primeiro em amostras reais e falsas. Ele aprende a identificar corretamente quais são reais e quais são falsas.\n",
    "* Gerador: Após o discriminador ser treinado, o gerador é treinado para melhorar a qualidade de suas amostras, baseando-se no feedback que recebe do discriminador.\n",
    "\n",
    "3. **Convergência:** Idealmente, o treinamento continua até que o discriminador não consiga mais distinguir entre amostras reais e geradas (ou seja, ambas têm 50% de chance de serem classificadas como reais), o que significa que o gerador está criando amostras realistas."
   ],
   "id": "8d8fd94236ba7916"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T12:10:16.313847Z",
     "start_time": "2024-08-25T12:10:16.306686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Função para treinar o modelo\n",
    "def train_dcgan(generator, discriminator, dataloader, criterion, optimizer_G, optimizer_D, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for i, (imgs, _) in enumerate(dataloader):\n",
    "            # Labels para real e falso\n",
    "            real_labels = torch.ones(imgs.size(0), 1).to(device)\n",
    "            fake_labels = torch.zeros(imgs.size(0), 1).to(device)\n",
    "\n",
    "            # Treinar o Discriminador\n",
    "            optimizer_D.zero_grad()\n",
    "            real_imgs = imgs.to(device)\n",
    "            real_loss = criterion(discriminator(real_imgs), real_labels)\n",
    "            z = torch.randn(imgs.size(0), latent_dim).to(device)\n",
    "            fake_imgs = generator(z)\n",
    "            fake_loss = criterion(discriminator(fake_imgs.detach()), fake_labels)\n",
    "            d_loss = real_loss + fake_loss\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Treinar o Gerador\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss = criterion(discriminator(fake_imgs), real_labels)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # Printar o progresso\n",
    "            if i % 400 == 0:\n",
    "                print(f\"Epoch [{epoch}/{epochs}] Batch {i}/{len(dataloader)} Loss D: {d_loss.item()}, Loss G: {g_loss.item()}\")\n",
    "\n",
    "        # Salvar algumas imagens geradas após cada época\n",
    "        save_image(fake_imgs[:25], f\"images_dgan/dcgan_{epoch}.png\", nrow=5, normalize=True)"
   ],
   "id": "7e8c7acc48d0bdee",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T12:12:11.717458Z",
     "start_time": "2024-08-25T12:12:11.712748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verificar e criar o diretório 'images' se ele não existir\n",
    "if not os.path.exists('images_dgan'):\n",
    "    os.makedirs('images_dgan')"
   ],
   "id": "d1d3c885e21ee45e",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T12:15:19.433551Z",
     "start_time": "2024-08-25T12:12:13.241739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Treinar o modelo\n",
    "train_dcgan(generator, discriminator, dataloader, criterion, optimizer_G, optimizer_D, epochs)"
   ],
   "id": "a4e58fdfb386096e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/20] Batch 0/938 Loss D: 0.1543407440185547, Loss G: 3.4584100246429443\n",
      "Epoch [0/20] Batch 400/938 Loss D: 0.1765507161617279, Loss G: 2.94061541557312\n",
      "Epoch [0/20] Batch 800/938 Loss D: 0.13682261109352112, Loss G: 3.237119674682617\n",
      "Epoch [1/20] Batch 0/938 Loss D: 0.14677605032920837, Loss G: 3.101118564605713\n",
      "Epoch [1/20] Batch 400/938 Loss D: 0.2020512968301773, Loss G: 2.817969799041748\n",
      "Epoch [1/20] Batch 800/938 Loss D: 0.2253589928150177, Loss G: 3.056427001953125\n",
      "Epoch [2/20] Batch 0/938 Loss D: 0.6829238533973694, Loss G: 6.409400939941406\n",
      "Epoch [2/20] Batch 400/938 Loss D: 0.2674337327480316, Loss G: 2.745242118835449\n",
      "Epoch [2/20] Batch 800/938 Loss D: 0.7125169634819031, Loss G: 6.037772178649902\n",
      "Epoch [3/20] Batch 0/938 Loss D: 0.22005322575569153, Loss G: 3.083742141723633\n",
      "Epoch [3/20] Batch 400/938 Loss D: 0.251420795917511, Loss G: 2.37601375579834\n",
      "Epoch [3/20] Batch 800/938 Loss D: 0.1888951063156128, Loss G: 3.6232614517211914\n",
      "Epoch [4/20] Batch 0/938 Loss D: 0.2053801417350769, Loss G: 3.6330413818359375\n",
      "Epoch [4/20] Batch 400/938 Loss D: 0.3553832769393921, Loss G: 5.043379306793213\n",
      "Epoch [4/20] Batch 800/938 Loss D: 0.2112089991569519, Loss G: 2.368603229522705\n",
      "Epoch [5/20] Batch 0/938 Loss D: 0.22918659448623657, Loss G: 3.372081756591797\n",
      "Epoch [5/20] Batch 400/938 Loss D: 0.8548526763916016, Loss G: 4.539126873016357\n",
      "Epoch [5/20] Batch 800/938 Loss D: 0.1890789121389389, Loss G: 2.7265279293060303\n",
      "Epoch [6/20] Batch 0/938 Loss D: 0.2518974840641022, Loss G: 3.1208627223968506\n",
      "Epoch [6/20] Batch 400/938 Loss D: 0.2899959981441498, Loss G: 3.263789653778076\n",
      "Epoch [6/20] Batch 800/938 Loss D: 0.13402876257896423, Loss G: 3.380211353302002\n",
      "Epoch [7/20] Batch 0/938 Loss D: 0.16525405645370483, Loss G: 3.7045652866363525\n",
      "Epoch [7/20] Batch 400/938 Loss D: 0.27041488885879517, Loss G: 5.306084632873535\n",
      "Epoch [7/20] Batch 800/938 Loss D: 0.29208338260650635, Loss G: 2.6131107807159424\n",
      "Epoch [8/20] Batch 0/938 Loss D: 0.13700173795223236, Loss G: 3.7022922039031982\n",
      "Epoch [8/20] Batch 400/938 Loss D: 0.17031055688858032, Loss G: 4.627191066741943\n",
      "Epoch [8/20] Batch 800/938 Loss D: 0.23555827140808105, Loss G: 3.2452611923217773\n",
      "Epoch [9/20] Batch 0/938 Loss D: 0.3579740822315216, Loss G: 2.5947036743164062\n",
      "Epoch [9/20] Batch 400/938 Loss D: 0.15978780388832092, Loss G: 3.403493881225586\n",
      "Epoch [9/20] Batch 800/938 Loss D: 0.2516472637653351, Loss G: 2.4965314865112305\n",
      "Epoch [10/20] Batch 0/938 Loss D: 0.22576500475406647, Loss G: 3.63846492767334\n",
      "Epoch [10/20] Batch 400/938 Loss D: 0.38625481724739075, Loss G: 4.037090301513672\n",
      "Epoch [10/20] Batch 800/938 Loss D: 0.191763773560524, Loss G: 3.885697841644287\n",
      "Epoch [11/20] Batch 0/938 Loss D: 0.14754831790924072, Loss G: 4.524816513061523\n",
      "Epoch [11/20] Batch 400/938 Loss D: 0.15166901051998138, Loss G: 3.0644607543945312\n",
      "Epoch [11/20] Batch 800/938 Loss D: 0.2639085054397583, Loss G: 3.0705649852752686\n",
      "Epoch [12/20] Batch 0/938 Loss D: 0.1472591906785965, Loss G: 3.3445651531219482\n",
      "Epoch [12/20] Batch 400/938 Loss D: 0.14570140838623047, Loss G: 3.924381732940674\n",
      "Epoch [12/20] Batch 800/938 Loss D: 0.1855771243572235, Loss G: 3.2541942596435547\n",
      "Epoch [13/20] Batch 0/938 Loss D: 0.64162677526474, Loss G: 5.583885669708252\n",
      "Epoch [13/20] Batch 400/938 Loss D: 1.1168336868286133, Loss G: 1.4373725652694702\n",
      "Epoch [13/20] Batch 800/938 Loss D: 0.21060514450073242, Loss G: 3.826418399810791\n",
      "Epoch [14/20] Batch 0/938 Loss D: 0.26696789264678955, Loss G: 4.170323848724365\n",
      "Epoch [14/20] Batch 400/938 Loss D: 0.2795480489730835, Loss G: 4.642293453216553\n",
      "Epoch [14/20] Batch 800/938 Loss D: 0.15368729829788208, Loss G: 4.512410640716553\n",
      "Epoch [15/20] Batch 0/938 Loss D: 0.23323747515678406, Loss G: 3.7921018600463867\n",
      "Epoch [15/20] Batch 400/938 Loss D: 0.152070090174675, Loss G: 3.151508331298828\n",
      "Epoch [15/20] Batch 800/938 Loss D: 0.15991836786270142, Loss G: 3.6837258338928223\n",
      "Epoch [16/20] Batch 0/938 Loss D: 0.47310343384742737, Loss G: 1.4889533519744873\n",
      "Epoch [16/20] Batch 400/938 Loss D: 0.21353033185005188, Loss G: 4.9562578201293945\n",
      "Epoch [16/20] Batch 800/938 Loss D: 0.3749278783798218, Loss G: 4.482624053955078\n",
      "Epoch [17/20] Batch 0/938 Loss D: 0.10407392680644989, Loss G: 3.962698221206665\n",
      "Epoch [17/20] Batch 400/938 Loss D: 0.18789848685264587, Loss G: 3.992837429046631\n",
      "Epoch [17/20] Batch 800/938 Loss D: 0.12126292288303375, Loss G: 3.802976608276367\n",
      "Epoch [18/20] Batch 0/938 Loss D: 0.3499620258808136, Loss G: 5.071455955505371\n",
      "Epoch [18/20] Batch 400/938 Loss D: 0.24402402341365814, Loss G: 3.7808544635772705\n",
      "Epoch [18/20] Batch 800/938 Loss D: 0.12664169073104858, Loss G: 3.425778865814209\n",
      "Epoch [19/20] Batch 0/938 Loss D: 0.1388196051120758, Loss G: 4.514730453491211\n",
      "Epoch [19/20] Batch 400/938 Loss D: 0.1651989072561264, Loss G: 4.6518778800964355\n",
      "Epoch [19/20] Batch 800/938 Loss D: 0.12945261597633362, Loss G: 3.7783455848693848\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Loss D (Perda do discriminador)\n",
    "------------------------------\n",
    "\n",
    "A Loss D mede o quão bem o discriminador está distinguindo entre as amostras reais e as amostras geradas pelo gerador. Especificamente, é a soma da perda ao classificar as amostras reais (que o discriminador deve classificar como reais) e as amostras falsas (que o discriminador deve classificar como falsas).\n",
    "\n",
    "* **No início do treinamento:** A Loss D geralmente começa alta, porque o gerador é inicializado com pesos aleatórios e, portanto, gera amostras que são facilmente distinguíveis como falsas.\n",
    "* **Durante o treinamento:** À medida que o gerador melhora, a Loss D pode começar a diminuir, pois o discriminador terá mais dificuldade em distinguir entre as amostras reais e as falsas. Idealmente, a Loss D deve se manter em torno de 0.5 quando o gerador e o discriminador estão equilibrados, indicando que o discriminador tem 50% de chance de classificar corretamente, ou seja, não consegue distinguir bem as amostras reais das falsas.\n",
    "* **Oscilações:** A Loss D pode oscilar, refletindo a \"batalha\" entre o gerador e o discriminador. Se a Loss D for muito baixa, o discriminador está muito bom; se for muito alta, o discriminador pode estar falhando em diferenciar as amostras.\n",
    "\n",
    "Loss G (Perda do Gerador)\n",
    "-------------------------\n",
    "\n",
    "A Loss G mede o quão bem o gerador está enganando o discriminador. Especificamente, é calculada tentando maximizar a probabilidade de o discriminador classificar as amostras falsas como reais.\n",
    "\n",
    "* **No início do treinamento:** A Loss G geralmente começa alta porque o gerador está gerando amostras aleatórias que o discriminador consegue identificar facilmente como falsas.\n",
    "* **Durante o treinamento:** À medida que o gerador melhora e começa a gerar amostras mais realistas, a Loss G deve diminuir, indicando que o gerador está enganando o discriminador com mais frequência. No entanto, se a Loss G for muito baixa, pode significar que o gerador está \"ganhando\" demais, o que não é desejável, pois pode indicar que o discriminador não está treinando de forma eficaz.\n",
    "* **Oscilações:** A Loss G também pode oscilar. Se a Loss G for muito alta, o gerador pode estar gerando amostras de baixa qualidade. Se for muito baixa, pode indicar que o discriminador não está aprendendo adequadamente.\n",
    "\n",
    "Equilíbrio entre `Loss D` e `Loss G`\n",
    "--------------------------------------\n",
    "\n",
    "* **Equilíbrio:** Idealmente, durante o treinamento, deve haver um equilíbrio dinâmico entre Loss D e Loss G. Em outras palavras, ambas as perdas não devem ser consistentemente muito baixas ou muito altas, mas devem oscilar em torno de valores intermediários. Isso indicaria que o gerador está constantemente melhorando para enganar o discriminador, enquanto o discriminador está constantemente melhorando para identificar as amostras falsas.\n",
    "\n",
    "* **Instabilidade:** Se uma perda (Loss D ou Loss G) se aproxima de 0 ou cresce sem limites, isso pode indicar instabilidade no treinamento. Por exemplo, se Loss D for quase 0, o discriminador está muito forte e o gerador pode não estar aprendendo nada útil. Se Loss G for quase 0, o gerador pode estar \"vencendo\" facilmente, e o discriminador pode não estar recebendo feedback útil para melhorar."
   ],
   "id": "2da25a2133bef3bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Mode Collapse\n",
    "----------------\n",
    "\n",
    "Às vezes, o gerador aprende a produzir uma quantidade limitada de amostras que enganam o discriminador repetidamente, levando a uma Loss G baixa, mas com baixa diversidade nas amostras geradas. Isso é conhecido como \"mode collapse\"."
   ],
   "id": "c1f6536aaaf731a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Treinamento instável\n",
    "--------------------\n",
    "\n",
    "As GANs são notórias por seu treinamento instável, onde o equilíbrio entre o gerador e o discriminador pode ser difícil de manter. Ajustes nos hiperparâmetros, como as taxas de aprendizado e o uso de técnicas de regularização, podem ser necessários para estabilizar o treinamento."
   ],
   "id": "db0288edb2d6157"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Resumo de monitoramento\n",
    "---------------------\n",
    "\n",
    "* **`Loss D:`** Idealmente, deve oscilar em torno de 0.5, indicando que o discriminador tem dificuldade em distinguir amostras reais das falsas.\n",
    "* **`Loss G:`** Deve diminuir ao longo do tempo, mas não deve ser muito baixa, indicando que o gerador está aprendendo a enganar o discriminador, mas não completamente.\n",
    "* **`Equilíbrio Dinâmico:`** O sucesso de uma GAN depende de um equilíbrio dinâmico entre Loss D e Loss G, onde ambos os modelos estão aprendendo de forma eficaz."
   ],
   "id": "6db89e84fe5b099e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
